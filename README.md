# MMSafety

## Inference Guidelines

### FigStep

```bash
python vllm_inference.py -cp conf/vllm/fig_step -cn llama3_text_test_v1_0
```

### Results

#### Text-only

|              Model               | FigStep | MM-Safety - RQ | MM-Safety - SD-TYPO+RQ | MM-Safety - SD+RQ-SD | MM-Safety - TYPO+RQ | VL-Guard | VL-Safe |
|:--------------------------------:|:-------:|:--------------:|:----------------------:|---------------------:|:-------------------:|:--------:|:-------:|
|        Llama3-8b-Instruct        |  71.4   |     20.71      |          ---           |                  --- |         ---         |  19.96   |  99.28  |
| Description + Llama3-8b-Instruct |         |      ---       |         52.44          |                31.19 |        60.89        |  27.28   |         |

----

##### VL-Guard

- Llama-3-8b-Instruct
    - Text-only
        - Overall Acc.: 54.11
        - Safe-image-safe-instruction Acc.: 97.67
        - Safe-image-unsafe-instruction Acc.: 18.10
        - Unsafe-image: 44.57
    - Description + Text
        - Overall Acc.: 60.08
        - Safe-image-safe-instruction Acc.: 97.31
        - Safe-image-unsafe-instruction Acc.: 27.59
        - Unsafe-image: 54.07

**Attack**

|                 Model | Acc.  | Safe-image-unsafe-instruction | safe-image-safe-instruction | unsafe-image |
|----------------------:|:-----:|:-----------------------------:|:---------------------------:|:------------:|
|         Llava-v1.5-7b | 42.62 |             12.54             |            98.57            |     9.95     |
|        Llava-v1.5-13b | 47.05 |             22.58             |            98.56            |    12.90     |
|  Llava-next-llama3-8b | 47.63 |             13.98             |            96.42            |    28.51     |
| Llava-v1.6-mistral-7b | 50.58 |             21.86             |            96.42            |    28.96     |
|     Phi-3-vision-128k | 68.10 |             43.73             |            94.98            |    64.93     |
|           InternVL-8B | 51.54 |             23.12             |            98.39            |    28.28     |

##### MM-Safety

|                                            Model | Overall Reject Rate | Illegal Activity | Hate Speech | Malware Generation | Physical Harm | Economic Harm | Fraud |  Sex  | Political Lobbying | Privacy Violence | Legal Opinion | Financial Advice | Health Consultation | Gov Decision |
|-------------------------------------------------:|:-------------------:|:----------------:|:-----------:|:------------------:|:-------------:|:-------------:|:-----:|:-----:|:------------------:|:----------------:|:-------------:|:----------------:|:-------------------:|:------------:|
|                               Llama3-8b-Instruct |        20.71        |      41.24       |    65.03    |       70.45        |     70.83     |     90.16     | 61.04 | 92.66 |       93.46        |      64.75       |     83.85     |      94.01       |         1.0         |    93.96     |
|            Llama3-8b-Instruct (w/ system prompt) |        34.17        |      71.13       |    53.37    |       43.18        |     32.64     |     11.48     | 48.70 | 10.09 |       16.99        |      41.73       |     52.31     |      13.17       |        14.68        |    41.61     |
|   Llama3-8b-Instruct SD+RQ-SD (w/ system prompt) |        47.08        |      78.35       |    58.90    |       81.81        |     49.31     |     24.59     | 51.95 | 13.76 |       23.53        |      43.17       |     66.15     |      27.54       |        49.54        |    70.47     |
|                                     ~~~~ + Guard |        47.56        |      78.35       |    58.90    |       81.81        |     52.08     |     24.59     | 51.95 | 13.76 |       24.84        |      43.88       |     66.92     |      27.54       |        49.54        |    70.47     |
| Llama3-8b-Instruct SD-TYPO+RQ (w/ system prompt) |        68.27        |      98.96       |    84.66    |       97.73        |     76.39     |     45.90     | 87.01 | 66.05 |       41.18        |      70.50       |     70.0      |      27.54       |        71.56        |    81.88     |
|                                     ~~~~ + Guard |        68.93        |      98.97       |    85.28    |       97.73        |     79.17     |     45.90     | 87.66 | 66.05 |       41.18        |      72.66       |     71.54     |      27.54       |        71.56        |    81.88     |                      
|    Llama3-8b-Instruct TYPO+RQ (w/ system prompt) |        73.93        |       100        |    92.64    |       97.72        |     83.33     |     45.90     | 93.51 | 88.99 |       38.56        |      89.93       |     76.15     |      29.34       |        73.39        |    81.88     | 
|                           Llava-v1.5-7b SD+RQ-SD |        13.39        |      28.87       |    13.50    |       15.91        |     17.36     |     8.20      | 11.04 | 7.34  |        2.61        |      18.71       |     24.62     |      14.37       |        15.60        |     3.36     |
|                         Llava-v1.5-7b SD-TYPO+RQ |        13.09        |      49.48       |    17.18    |       15.91        |     14.58     |     7.38      | 22.08 | 4.59  |        3.9         |      20.14       |     11.54     |       5.39       |        8.26         |     0.67     |
|                            Llava-v1.5-7b TYPO+RQ |        12.92        |      32.99       |    17.79    |        9.09        |     19.44     |     8.20      | 16.23 | 4.58  |        3.27        |      11.51       |      20       |       9.58       |        12.84        |     4.70     |
|                          Llava-v1.5-13b SD+RQ-SD |        12.80        |      35.05       |    10.43    |       18.18        |     17.36     |     8.20      | 13.64 | 6.42  |         0          |      12.95       |     23.85     |      13.17       |        15.60        |     3.36     |
|                        Llava-v1.5-13b SD-TYPO+RQ |        21.49        |      73.20       |    27.61    |       29.55        |     33.33     |     14.11     | 35.06 | 7.34  |        1.96        |      32.37       |     23.08     |      12.57       |        4.59         |     1.34     |
|                           Llava-v1.5-13b TYPO+RQ |        15.83        |      47.42       |    17.18    |       15.91        |     18.06     |     5.74      | 19.48 | 7.34  |        3.92        |      15.83       |     25.38     |      14,97       |        19.27        |     4.70     |
|                    Llava-next-llama3-8b SD+RQ-SD |        23.57        |      44.33       |    26.99    |       29.55        |     27.08     |     18.85     | 25.97 | 11.00 |        5.88        |      18.71       |      20       |      40.72       |        24.77        |    17.45     |
|                  Llava-next-llama3-8b SD-TYPO+RQ |        42.68        |      77.32       |    51.53    |       56.82        |     59/02     |     36.07     | 59.74 | 31.19 |        5.88        |      55.40       |     31.54     |      35.93       |        31.28        |    30.87     |
|                     Llava-next-llama3-8b TYPO+RQ |        46.19        |      83.51       |    57.06    |       68.18        |     63.89     |     39.34     | 70.13 | 32.11 |        5.88        |      56.83       |     36.92     |      40.72       |        49.45        |    28.19     |
|                   Llava-v1.6-mistral-7b SD+RQ-SD |        20.59        |      46.39       |    22.09    |       20.45        |     18.75     |     15.57     | 23.38 | 11.01 |        5.23        |      15.83       |     25.38     |      25.15       |        33.94        |    13.42     |
|                 Llava-v1.6-mistral-7b SD-TYPO+RQ |        37.79        |      77.32       |    48.47    |       59.09        |     40.27     |     26.23     | 62.34 | 22.02 |        3.92        |      51.08       |     25.38     |      27.54       |        55.05        |    19.46     |
|                    Llava-v1.6-mistral-7b TYPO+RQ |        42.38        |      92.78       |    53.37    |       63.63        |     57.64     |     29.51     | 67.53 | 25.69 |        7.19        |      55.40       |     40.00     |      25.15       |        43.12        |    18.12     |
|                       Phi-3-Vision-128k SD+RQ-SD |        68.99        |      80.41       |    85.89    |       59.09        |     64.58     |     58.20     | 72.08 | 71.56 |       51.63        |      74.82       |     69.23     |      62.28       |        84.40        |    62.41     | 
|                      Phi-3-Vision-128 SD-TYPO+RQ |        79.88        |      97.93       |    98.16    |       86.36        |     90.28     |     83.61     | 96.75 | 75.23 |       90.85        |      96.40       |     61.54     |      40.12       |        67.89        |    61.74     |
|                        Phi-3-Vision-128k TYPO+RQ |        77.86        |      94.85       |    96.93    |       84.09        |     88.89     |     75.41     | 95.45 | 53.21 |       83.01        |      97.84       |     70.77     |      46.71       |        67.89        |    59.73     |  
|                             InternVL-8B SD+RQ-SD |        31.19        |      52.58       |    36.81    |       27.27        |     38.19     |     26.23     | 44.16 | 12.84 |       10.46        |      39.57       |     32.31     |      40.12       |        28.44        |    14.09     |
|                           InternVL-8B SD-TYPO+RQ |        41.25        |      90.72       |    48.47    |         50         |     59.72     |     32.79     | 66.88 | 14.68 |       15.03        |      61.87       |     32.31     |      37.72       |        22.02        |    14.09     |
|                              InternVL-8B TYPO+RQ |        46.96        |      92.78       |    66.26    |       56.82        |     58.33     |     32.79     | 79.87 | 29.36 |       16.34        |      70.50       |     40.77     |      36.53       |        24.77        |    15.44     |

##### FigStep

|                 Model | Overall | Illegal Activity | Hate Speech | Malware Generation | Physical Harm | Fraud | Adult Content | Privacy Violation | Legal Opinion | Financial Advice | Health Consultation | 
|----------------------:|:-------:|:----------------:|:-----------:|:------------------:|:-------------:|:-----:|:-------------:|:-----------------:|:-------------:|:----------------:|:-------------------:|
|         Llava-v1.5-7b |  34.40  |      52.00       |    50.00    |       58.00        |     38.00     | 42.00 |     20.00     |       26.00       |     14.00     |      18.00       |        26.00        |
|        Llava-v1.5-13b |  46.80  |       72.0       |    62.0     |        76.0        |     60.0      | 52.0  |     24.0      |       42.0        |     22.0      |       20.0       |        38.0         |
|  Llava-next-llama3-8b |  51.6   |       72.0       |    50.0     |        80.0        |     76.0      | 76.0  |     26.0      |       42.0        |     28.0      |       36.0       |        30.0         |
| Llava-v1.6-mistral-7b |  49.8   |       84.0       |    70.0     |        80.0        |     66.0      | 74.0  |     28.0      |       42.0        |     16.0      |       22.0       |        16.0         |
|           InternVL-8B |  54.2   |       56.0       |    86.0     |        84.0        |     70.0      | 82.0  |     16.0      |       66.0        |     26.0      |       22.0       |        34.0         | 
|     Phi-3-vision-128k |  89.2   |       94.0       |    100.0    |        96.0        |     82.0      | 86.0  |     70.0      |       96.0        |     72.0      |       88.0       |        88.0         |

##### VL-Safe

|                 Model | Acc.  | 
|----------------------:|:-----:|
|         Llava-v1.5-7b | 41.72 | 
|        Llava-v1.5-13b | 41.53 |  
|  Llava-next-llama3-8b | 53.06 |   
| Llava-v1.6-mistral-7b | 84.59 |    
|     Phi-3-vision-128k | 80.36 |
|           InternVL-8B | 74.59 |

