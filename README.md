# MMSafety

## Inference Guidelines

### FigStep

```bash
python vllm_inference.py -cp conf/vllm/fig_step -cn llama3_text_test_v1_0
```

### Results

#### Text-only

|              Model               | FigStep | MM-Safety - RQ | MM-Safety - SD-TYPO+RQ | MM-Safety - SD+RQ-SD | MM-Safety - TYPO+RQ | VL-Guard | VL-Safe |
|:--------------------------------:|:-------:|:--------------:|:----------------------:|---------------------:|:-------------------:|:--------:|:-------:|
|        Llama3-8b-Instruct        |  71.4   |     20.71      |          ---           |                  --- |         ---         |  19.96   |  99.28  |
| Description + Llama3-8b-Instruct |         |      ---       |         52.44          |                31.19 |        60.89        |  27.28   |         |

----

##### VL-Guard

- Llama-3-8b-Instruct
    - Text-only
        - Overall Acc.: 54.11
        - Safe-image-safe-instruction Acc.: 97.67
        - Safe-image-unsafe-instruction Acc.: 18.10
        - Unsafe-image: 44.57
    - Description + Text
        - Overall Acc.: 60.08
        - Safe-image-safe-instruction Acc.: 97.31
        - Safe-image-unsafe-instruction Acc.: 27.59
        - Unsafe-image: 54.07

**Attack**

|                 Model | Acc.  | Safe-image-unsafe-instruction | safe-image-safe-instruction | unsafe-image |
|----------------------:|:-----:|:-----------------------------:|:---------------------------:|:------------:|
|         Llava-v1.5-7b | 42.62 |             12.54             |            98.57            |     9.95     |
|        Llava-v1.5-13b | 47.05 |             22.58             |            98.56            |    12.90     |
|  Llava-next-llama3-8b | 47.63 |             13.98             |            96.42            |    28.51     |
| Llava-v1.6-mistral-7b | 50.58 |             21.86             |            96.42            |    28.96     |

##### MM-Safety

|                      Model                       | Overall Reject Rate | Illegal Activity | Hate Speech | Malware Generation | Physical Harm | Economic Harm | Fraud |  Sex  | Political Lobbying | Privacy Violence | Legal Opinion | Financial Advice | Health Consultation | Gov Decision |
|:------------------------------------------------:|:-------------------:|:----------------:|:-----------:|:------------------:|:-------------:|:-------------:|:-----:|:-----:|:------------------:|:----------------:|:-------------:|:----------------:|:-------------------:|:------------:|
|                Llama3-8b-Instruct                |        20.71        |      41.24       |    65.03    |       70.45        |     70.83     |     90.16     | 61.04 | 92.66 |       93.46        |      64.75       |     83.85     |      94.01       |         1.0         |    93.96     |
|      Llama3-8b-Instruct (w/ system prompt)       |        34.17        |      71.13       |    53.37    |       43.18        |     32.64     |     11.48     | 48.70 | 10.09 |       16.99        |      41.73       |     52.31     |      13.17       |        14.68        |    41.61     |
|  Llama3-8b-Instruct SD+RQ-SD (w/ system prompt)  |        47.08        |      78.35       |    58.90    |       81.81        |     49.31     |     24.59     | 51.95 | 13.76 |       23.53        |      43.17       |     66.15     |      27.54       |        49.54        |    70.47     |
|                   ~~~~ + Guard                   |        47.56        |      78.35       |    58.90    |       81.81        |     52.08     |     24.59     | 51.95 | 13.76 |       24.84        |      43.88       |     66.92     |      27.54       |        49.54        |    70.47     |
| Llama3-8b-Instruct SD-TYPO+RQ (w/ system prompt) |        68.27        |      98.96       |    84.66    |       97.73        |     76.39     |     45.90     | 87.01 | 66.05 |       41.18        |      70.50       |     70.0      |      27.54       |        71.56        |    81.88     |
|                   ~~~~ + Guard                   |        68.93        |      98.97       |    85.28    |       97.73        |     79.17     |     45.90     | 87.66 | 66.05 |       41.18        |      72.66       |     71.54     |      27.54       |        71.56        |    81.88     |                      
|  Llama3-8b-Instruct TYPO+RQ (w/ system prompt)   |        73.93        |       100        |    92.64    |       97.72        |     83.33     |     45.90     | 93.51 | 88.99 |       38.56        |      89.93       |     76.15     |      29.34       |        73.39        |    81.88     | 
|              Llava-v1.5-7b SD+RQ-SD              |        13.39        |      28.87       |    13.50    |       15.91        |     17.36     |     8.20      | 11.04 | 7.34  |        2.61        |      18.71       |     24.62     |      14.37       |        15.60        |     3.36     |
|             Llava-v1.5-7b SD-TYPO+RQ             |        13.09        |      49.48       |    17.18    |       15.91        |     14.58     |     7.38      | 22.08 | 4.59  |        3.9         |      20.14       |     11.54     |       5.39       |        8.26         |     0.67     |
|              Llava-v1.5-7b TYPO+RQ               |        12.92        |      32.99       |    17.79    |        9.09        |     19.44     |     8.20      | 16.23 | 4.58  |        3.27        |      11.51       |      20       |       9.58       |        12.84        |     4.70     |
|             Llava-v1.5-13b SD+RQ-SD              |        12.80        |      35.05       |    10.43    |       18.18        |     17.36     |     8.20      | 13.64 | 6.42  |         0          |      12.95       |     23.85     |      13.17       |        15.60        |     3.36     |
|            Llava-v1.5-13b SD-TYPO+RQ             |        21.49        |      73.20       |    27.61    |       29.55        |     33.33     |     14.11     | 35.06 | 7.34  |        1.96        |      32.37       |     23.08     |      12.57       |        4.59         |     1.34     |
|              Llava-v1.5-13b TYPO+RQ              |        15.83        |      47.42       |    17.18    |       15.91        |     18.06     |     5.74      | 19.48 | 7.34  |        3.92        |      15.83       |     25.38     |      14,97       |        19.27        |     4.70     |
|          Llava-next-llama3-8b SD+RQ-SD           |        23.57        |      44.33       |    26.99    |       29.55        |     27.08     |     18.85     | 25.97 | 11.00 |        5.88        |      18.71       |      20       |      40.72       |        24.77        |    17.45     |
|         Llava-next-llama3-8b SD-TYPO+RQ          |        42.68        |      77.32       |    51.53    |       56.82        |     59/02     |     36.07     | 59.74 | 31.19 |        5.88        |      55.40       |     31.54     |      35.93       |        31.28        |    30.87     |
|           Llava-next-llama3-8b TYPO+RQ           |        46.19        |      83.51       |    57.06    |       68.18        |     63.89     |     39.34     | 70.13 | 32.11 |        5.88        |      56.83       |     36.92     |      40.72       |        49.45        |    28.19     |
|          Llava-v1.6-mistral-7b SD+RQ-SD          |        20.59        |      46.39       |    22.09    |       20.45        |     18.75     |     15.57     | 23.38 | 11.01 |        5.23        |      15.83       |     25.38     |      25.15       |        33.94        |    13.42     |
|         Llava-v1.6-mistral-7b SD-TYPO+RQ         |        37.79        |      77.32       |    48.47    |       59.09        |     40.27     |     26.23     | 62.34 | 22.02 |        3.92        |      51.08       |     25.38     |      27.54       |        55.05        |    19.46     |
|          Llava-v1.6-mistral-7b TYPO+RQ           |        42.38        |      92.78       |    53.37    |       63.63        |     57.64     |     29.51     | 67.53 | 25.69 |        7.19        |      55.40       |     40.00     |      25.15       |        43.12        |    18.12     |

##### FigStep

|                 Model | Overall | Illegal Activity | Hate Speech | Malware Generation | Physical Harm | Fraud | Adult Content | Privacy Violation | Legal Opinion | Financial Advice | Health Consultation | 
|----------------------:|:-------:|:----------------:|:-----------:|:------------------:|:-------------:|:-----:|:-------------:|:-----------------:|:-------------:|:----------------:|:-------------------:|
|         Llava-v1.5-7b |  34.40  |      52.00       |    50.00    |       58.00        |     38.00     | 42.00 |     20.00     |       26.00       |     14.00     |      18.00       |        26.00        |
|        Llava-v1.5-13b |  46.80  |       72.0       |    62.0     |        76.0        |     60.0      | 52.0  |     24.0      |       42.0        |     22.0      |       20.0       |        38.0         |
|  Llava-next-llama3-8b |  51.6   |       72.0       |    50.0     |        80.0        |     76.0      | 76.0  |     26.0      |       42.0        |     28.0      |       36.0       |        30.0         |
| Llava-v1.6-mistral-7b |  49.8   |       84.0       |    70.0     |        80.0        |     66.0      | 74.0  |     28.0      |       42.0        |     16.0      |       22.0       |        16.0         |


##### VL-Safe


|                 Model | Acc.  | 
|----------------------:|:-----:|
|         Llava-v1.5-7b | 41.72 | 
|        Llava-v1.5-13b | 41.53 |  
|  Llava-next-llama3-8b | 53.06 |   
| Llava-v1.6-mistral-7b | 84.59 |    


